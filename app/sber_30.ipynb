{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ae1834-ae30-4799-8aa4-5786ee49d845",
   "metadata": {},
   "source": [
    "### Baseline модель для определения именованных сущностей по кейсу от Rutube.\n",
    "Поскольку нам нужно распознать нестандартные NER, можно воспользоваться помощью языковых моделей, в данном случае - Bert.\n",
    "Данные вы уже получили  - это разметка, сделанная на Толоке, она не идеальна, но это часть практической задачи, с которой можно столкнуться в реальности. \n",
    "\n",
    "Небольшое введение в NER https://habr.com/ru/companies/contentai/articles/449514/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f651cc-6e8c-431c-a84e-2e7d310c9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] \n",
    "#!pip install datasets seqeval\n",
    "#!pip install corus razdel \n",
    "#!pip install torch==2.0.0\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86789ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# считаем данные\n",
    "import os \n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 2\n",
    "num_train_epochs = 30\n",
    "model_checkpoint = \"xlm-roberta-large\"\n",
    "name_folder_and_model = 'xlm-roberta-large_30.bin'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)\n",
    "data = pd.read_csv(\"ner_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902ad843-b9a6-4aeb-b283-6590444aea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные спарсены с Толоки, поэтому могут иметь проблемы с символами и их нужно избежать, \n",
    "# удалить лишние '\\' например, преобразовать из str в список dict-ов\n",
    "import json\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7c226-ba06-4b25-8e7c-1423f3d49e00",
   "metadata": {},
   "source": [
    "#### Оригинал туториала на медицинских данных можно посмотреть тут https://gist.github.com/avidale/cacf235aebeaaf4c578389e1146c3c57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee336880-48fd-4c3c-9b84-4f9487d5d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь из наших данных нам нужно извлечь для каждого слова (токена) его тег (label) из разметки, чтобы потом предать в модель классификации токенов\n",
    "from razdel import tokenize\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # воспользуемся удобным токенайзером из библиотеки razdel, \n",
    "    # она помимо разбиения на слова, сохраняет важные для нас числа - начало и конец слова в токенах\n",
    "    \n",
    "    raw_toks = list(tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # присвоим для начала каждому слову тег 'О' - тег, означающий отсутствие NER-а\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # так как NER можем состаять из нескольких слов, то нам нужно сохранить эту инфорцию\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != 'не найдено':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43db9d6-76f9-4f24-87ce-bea724a238a9",
   "metadata": {},
   "source": [
    "### Обработаем датасет и разобьем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea76082-efcc-44da-ba3c-7dd466d3ecb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f181346-579e-4804-975a-f6133a86a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc651f-f5f7-48f6-82d3-acfbb38a5296",
   "metadata": {},
   "source": [
    "#### Посмотрим на получившиеся теги\n",
    "Подробнее почитать про BIO теги можно тут https://datascience.stackexchange.com/questions/63399/what-is-bio-tags-for-creating-custom-ner-named-entity-recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d154c93-c038-4c4a-b9ee-8dba7bd1a21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Дата',\n",
       " 'B-бренд',\n",
       " 'B-вид спорта',\n",
       " 'B-видеоигра',\n",
       " 'B-команда',\n",
       " 'B-лига',\n",
       " 'B-локация',\n",
       " 'B-модель',\n",
       " 'B-название проекта',\n",
       " 'B-организация',\n",
       " 'B-персона',\n",
       " 'B-сезон',\n",
       " 'B-серия',\n",
       " 'I-Дата',\n",
       " 'I-бренд',\n",
       " 'I-вид спорта',\n",
       " 'I-видеоигра',\n",
       " 'I-команда',\n",
       " 'I-лига',\n",
       " 'I-локация',\n",
       " 'I-модель',\n",
       " 'I-название проекта',\n",
       " 'I-организация',\n",
       " 'I-персона',\n",
       " 'I-сезон',\n",
       " 'I-серия']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803e14bf-5ca9-42bf-8c67-f3718b53d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4612e752-dc7d-41e6-bede-73e67319aa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5137\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 1285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd286f0c-d748-4289-bf3e-8064e45186cc",
   "metadata": {},
   "source": [
    "### Запустим модель RuBert-tiny - классический Bert, поверх которого навешен слой классификации токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417623f9-c9ac-488f-9e1b-31dbf61e049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58c25f2-b144-4b0c-9dd6-76760d26152f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'НАЗВАНИЕ', ':', '>', 'Московский', 'международный', 'фестиваль', 'мира', '=', '89', 'и', 'Стас', 'Намин', '.', '«', 'Главный', 'день', '»', '<', 'ОПИСАНИЕ', ':', '>', '12', 'и', '13', 'июня', '1989', 'года', 'на', 'Центральном', 'стадионе', 'имени', 'В', '.', 'И', '.', 'Ленина', 'состоялся', 'первый', 'международный', 'рок', 'фестиваль', '.', 'Концерт', 'собрал', 'свыше', '100', 'тысяч', 'зрителей', '.', 'Советская', 'молодежь', 'тогда', 'увидела', 'и', 'услышала', 'признанных', 'мировых', 'кумиров', ':', 'Scorpions', ',', 'Bon', 'Jovi', ',', 'Ozzy', 'Osbourne', ',', 'Cinderella', ',', 'Motley', 'Crue', '.', 'Приезд', 'западных', 'звезд', 'казался', 'ожившей', 'фантастикой', '.', 'Фестиваль', 'стал', 'не', 'только', 'музыкальной', ',', 'но', 'и', 'важной', 'политической', 'акцией', '.', 'Вся', 'мировая', 'пресса', 'писала', 'о', 'том', ',', 'что', 'СССР', 'и', 'США', ',', 'долгое', 'время', 'находившиеся', 'в', 'состоянии', '\"', 'холодной', 'войны', '\"', ',', 'наконец', 'то', 'стали', 'налаживать', 'дружественные', 'отношения', 'на', 'культурном', 'фронте', '.', 'Приезд', 'западных', 'музыкантов', 'разрушил', 'мифы', 'и', 'стереотипы', 'о', 'Советском', 'Союзе', ',', 'как', 'о', 'суровой', 'и', 'закрытой', 'стране', '.', 'Но', 'что', 'осталось', 'за', 'кадром', 'этой', 'музыкальной', 'революции', '?', 'Главный', 'организатор', 'фестиваля', 'музыкант', 'и', 'продюсер', 'Стас', 'Намин', ',', 'а', 'также', 'некоторые', 'участники', 'согласились', 'поделиться', 'своими', 'личными', 'воспоминаниями', '.']\n"
     ]
    }
   ],
   "source": [
    "example = ner_train[5]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bf5c30-7ee2-46d3-9733-da3ff8b803fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁<', '▁НА', 'З', 'ВА', 'НИЕ', '▁:', '▁>', '▁Московск', 'ий', '▁между', 'народ', 'ный', '▁фестиваль', '▁мира', '▁=', '▁89', '▁и', '▁Ста', 'с', '▁На', 'мин', '▁', '.', '▁«', '▁Глав', 'ный', '▁день', '▁»', '▁<', '▁О', 'ПИС', 'А', 'НИЕ', '▁:', '▁>', '▁12', '▁и', '▁13', '▁июня', '▁1989', '▁года', '▁на', '▁Центральн', 'ом', '▁стадион', 'е', '▁имени', '▁В', '▁', '.', '▁И', '▁', '.', '▁Ленин', 'а', '▁состоя', 'лся', '▁первый', '▁между', 'народ', 'ный', '▁рок', '▁фестиваль', '▁', '.', '▁Концерт', '▁со', 'брал', '▁свыше', '▁100', '▁тысяч', '▁зрител', 'ей', '▁', '.', '▁Совет', 'ская', '▁молод', 'еж', 'ь', '▁тогда', '▁увидел', 'а', '▁и', '▁услышал', 'а', '▁призна', 'нных', '▁мир', 'овых', '▁ку', 'мир', 'ов', '▁:', '▁S', 'corp', 'ions', '▁', ',', '▁Bon', '▁Jo', 'vi', '▁', ',', '▁O', 'zzy', '▁Os', 'bour', 'ne', '▁', ',', '▁C', 'inde', 'rella', '▁', ',', '▁Mot', 'ley', '▁Cru', 'e', '▁', '.', '▁При', 'езд', '▁запад', 'ных', '▁звезд', '▁каза', 'лся', '▁ожи', 'вшей', '▁фантасти', 'кой', '▁', '.', '▁Фестивал', 'ь', '▁стал', '▁не', '▁только', '▁музыка', 'льной', '▁', ',', '▁но', '▁и', '▁важно', 'й', '▁политической', '▁ак', 'цией', '▁', '.', '▁В', 'ся', '▁мир', 'овая', '▁пресс', 'а', '▁писал', 'а', '▁о', '▁том', '▁', ',', '▁что', '▁СССР', '▁и', '▁США', '▁', ',', '▁долго', 'е', '▁время', '▁находи', 'вшиеся', '▁в', '▁состоянии', '▁\"', '▁холодно', 'й', '▁войны', '▁\"', '▁', ',', '▁наконец', '▁то', '▁стали', '▁на', 'ла', 'живать', '▁', 'друже', 'ствен', 'ные', '▁отношения', '▁на', '▁культур', 'ном', '▁фронт', 'е', '▁', '.', '▁При', 'езд', '▁запад', 'ных', '▁музыкант', 'ов', '▁разруши', 'л', '▁ми', 'ф', 'ы', '▁и', '▁стереотип', 'ы', '▁о', '▁Совет', 'ском', '▁Союз', 'е', '▁', ',', '▁как', '▁о', '▁сур', 'овой', '▁и', '▁закрыт', 'ой', '▁стране', '▁', '.', '▁Но', '▁что', '▁осталось', '▁за', '▁кадр', 'ом', '▁этой', '▁музыка', 'льной', '▁революции', '▁?', '▁Глав', 'ный', '▁организатор', '▁фестивал', 'я', '▁музыкант', '▁и', '▁продюсер', '▁Ста', 'с', '▁На', 'мин', '▁', ',', '▁а', '▁также', '▁некоторые', '▁участники', '▁согласи', 'лись', '▁подели', 'ться', '▁своими', '▁ли', 'чными', '▁воспоминания', 'ми', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4163cecf-a14b-4c26-9273-3fa400055163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 276)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2321f2-8b2e-4178-bbd8-0b0a803aedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 1, 1, 2, 3, 4, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 13, 14, 15, 15, 16, 17, 18, 19, 19, 19, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 30, 31, 32, 33, 33, 34, 35, 35, 36, 36, 37, 37, 38, 39, 39, 39, 40, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 48, 49, 49, 50, 50, 51, 51, 51, 52, 53, 53, 54, 55, 55, 56, 56, 57, 57, 58, 58, 58, 59, 60, 60, 60, 61, 61, 62, 63, 63, 64, 64, 65, 65, 66, 66, 66, 67, 67, 68, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 73, 74, 74, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 82, 83, 84, 84, 85, 85, 86, 87, 88, 88, 89, 90, 90, 91, 91, 92, 92, 93, 93, 94, 94, 95, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 103, 104, 104, 105, 106, 106, 107, 108, 109, 110, 110, 111, 112, 113, 113, 114, 115, 116, 117, 117, 117, 118, 118, 118, 118, 119, 120, 121, 121, 122, 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 128, 129, 130, 130, 131, 132, 132, 133, 133, 134, 134, 135, 136, 137, 137, 138, 139, 139, 140, 141, 141, 142, 143, 144, 145, 146, 146, 147, 148, 148, 149, 150, 151, 151, 152, 153, 153, 154, 155, 156, 157, 157, 158, 158, 159, 159, 160, 161, 162, 163, 164, 164, 165, 165, 166, 167, 167, 168, 168, 169, 169, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17363352-e562-450a-a54f-f30cf6a3ede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 276\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa21220-e351-4259-83c5-c60cdb73dfe2",
   "metadata": {},
   "source": [
    "#### У Bert свой собсвенный токенайзер, который разбивает слова на мелкие токены, поэтому нам нужно корректно сопоставить токены и соответсвующие им неры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cb5952-a6aa-4032-8940-ec397d2bb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2735bc96-92cc-4361-8c44-583eabf4bb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 4426, 5315, 4026, 29889, 189182, 152, 977, 51208, 75586, 54039, 6, 83389, 45799, 1270, 102356, 4426, 1089, 157933, 1709, 189182, 152, 977, 123811, 336, 10698, 4558, 8568, 111529, 89, 69361, 19776, 100347, 292, 51208, 75586, 54039, 6, 83389, 45799, 1270, 102356, 6, 5, 417, 3920, 8165, 47745, 90216, 10041, 13549, 4476, 6, 5, 1089, 25358, 98590, 35, 98465, 100347, 1214, 129, 49571, 546, 135, 170769, 146408, 131567, 6, 47871, 1435, 8113, 1560, 8568, 129, 184862, 312, 1089, 88691, 89, 93362, 419, 93834, 17968, 63781, 6, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_data['train'][1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39acdcaf-7f6b-467f-a143-9820b4f13ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bb2a708c1d4b6db895c16adeb103cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257a26f56206480d8f3672e48bb71bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce292-6c29-43be-909b-adb1536b6ebf",
   "metadata": {},
   "source": [
    "#### Сохраняем словарик соотвествия тега и его индекса внутри модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edcbfc0-6bbd-4be1-9da7-6fe6e348753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7118790-3483-4fca-b3d9-950287202381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Специальный объект для удобного формирования батчей\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbfb44-68cc-42d1-98da-1a12a87aac10",
   "metadata": {},
   "source": [
    "### В качестве метрик возьмем precision, recall, accuracy, для этого воспользуемся специализированной под Ner задачу библиотеку seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc2483f-93f8-4207-92e2-1f91217a42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19660\\152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73d999fc-9eeb-4cd3-b358-762fdfd80f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бренд': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'локация': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_train[4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac26e3f4-52f8-4d98-b629-cd2b32fd8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1dbfdd9-443e-4b5e-90a7-e9abac9d4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='643' max='643' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [643/643 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.375157356262207,\n",
       " 'eval_precision': 0.004121586810922205,\n",
       " 'eval_recall': 0.0381474949376184,\n",
       " 'eval_f1': 0.007439395676488197,\n",
       " 'eval_accuracy': 0.009479054211384178,\n",
       " 'eval_runtime': 35.7511,\n",
       " 'eval_samples_per_second': 35.943,\n",
       " 'eval_steps_per_second': 17.985}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# что мы видим без дообучения модели\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9441d5bb-9bf3-4f3d-ad2d-24e577ed5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers.trainer import logger as noisy_logger\n",
    "noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf85ec22-19e5-4e10-be46-bf99fb57a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для дообучения берта можно эксперементировать с заморозкой/разморозкой разных слоев, здесь мы оставим все слои размороженными \n",
    "# Для быстроты обучения можно заморозить всю бертовую часть, кроме классификатора, но тогда качесвто будет похуже\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b907071-d37c-4d5b-8ce6-57aea13663bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'max_split_size_mb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_split_size_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'max_split_size_mb'"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5195e9a2-2c31-47f0-b539-512c486e99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90d12a5c-639b-4d84-a8c0-92b52b3cdf60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.17 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\transformers\\trainer.py:1971\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1969\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1972\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\accelerate\\optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\adamw.py:160\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\adamw.py:118\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    114\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    115\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    124\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    125\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.17 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a36b49-d3d3-4dcc-bb33-5b3cbdb555ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edf3c3-70e7-49c4-a9b6-842a0eb4f014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Посчитаем метрики на отложенном датасете\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f349-9ba5-42fe-8f5f-780c6f26d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead2c66-11d0-4cb8-ab58-530d2524b941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd985f-b4f6-4e6f-b28a-a54bf2e59c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(name_folder_and_model)\n",
    "tokenizer.save_pretrained(name_folder_and_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea14342-bbf6-40c0-aa44-3c2db9c8215b",
   "metadata": {},
   "source": [
    "### Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519aaaf-02b8-4f66-885c-8930073508a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ' '.join(ner_train[25]['tokens'])\n",
    "text = ner_train[25]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee5bc1-634c-4bde-aa51-64425aba5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device='cpu')\n",
    "\n",
    "def predict_ner(text, tokenizer, model, pipe, verbose=True):\n",
    "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens)\n",
    "    # print(pred.logits.shape)\n",
    "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    labels = []\n",
    "    for t, idx in zip(token_text, indices):\n",
    "        if '##' not in t:\n",
    "            labels.append(label_list[idx])\n",
    "        if verbose:    \n",
    "            print(f'{t:15s} {label_list[idx]:10s}')\n",
    "    return text, pipe(text), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a3a44-a23d-4615-8885-63da60761d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ner(text, tokenizer, model, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f580a-4542-42ea-98f4-de4a89fed35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b333764e-e82d-4c8f-9f89-8390f92da7cb",
   "metadata": {},
   "source": [
    "### Тестового датасета у вас пока нет, по которому будет считаться метрика на лидерборде, но прогоним для примера через нашу отложенную выборку, чтобы понять формат выходных данных.\n",
    "ВАЖНО: в тестовом датасете у вас будет тест в том же формате, что он был в трейне 'video_info', в финальном сабмишене эту колонку и индексы менять нельзя, нужно будет только заполнить колонку 'entities_prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ccfdd-46aa-49a0-af51-c7adea1d0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "submission = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
    "submission['entities_prediction'] = submission['entities_prediction'].astype('object')\n",
    "def sample_submission(text, tokenizer, model, pipe, submission):\n",
    "    for i, elem in enumerate(ner_test):\n",
    "        _, _, labels = predict_ner(elem['tokens'], tokenizer, model, pipe, verbose=False)\n",
    "        submission.loc[i, 'video_info'] = elem\n",
    "\n",
    "        submission.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958310b4-09fa-41bd-b888-b4364be3f397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = sample_submission(text, tokenizer, model, pipe, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0588b1-6e42-4f37-bfe6-2f695cb6971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdcc7-3750-46ad-abea-36fc712feb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90948bec-8675-46a0-8e6b-df159b8e4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dcd75-30bc-4bac-9d5c-a5ee879bd3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
