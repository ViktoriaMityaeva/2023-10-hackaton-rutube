{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ae1834-ae30-4799-8aa4-5786ee49d845",
   "metadata": {},
   "source": [
    "### Baseline –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –∫–µ–π—Å—É –æ—Ç Rutube.\n",
    "–ü–æ—Å–∫–æ–ª—å–∫—É –Ω–∞–º –Ω—É–∂–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ NER, –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ - Bert.\n",
    "–î–∞–Ω–Ω—ã–µ –≤—ã —É–∂–µ –ø–æ–ª—É—á–∏–ª–∏  - —ç—Ç–æ —Ä–∞–∑–º–µ—Ç–∫–∞, —Å–¥–µ–ª–∞–Ω–Ω–∞—è –Ω–∞ –¢–æ–ª–æ–∫–µ, –æ–Ω–∞ –Ω–µ –∏–¥–µ–∞–ª—å–Ω–∞, –Ω–æ —ç—Ç–æ —á–∞—Å—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–Ω–æ —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. \n",
    "\n",
    "–ù–µ–±–æ–ª—å—à–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ –≤ NER https://habr.com/ru/companies/contentai/articles/449514/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f651cc-6e8c-431c-a84e-2e7d310c9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] \n",
    "#!pip install datasets seqeval\n",
    "#!pip install corus razdel \n",
    "#!pip install torch==2.0.0\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86789ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# —Å—á–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "import os \n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 2\n",
    "num_train_epochs = 30\n",
    "model_checkpoint = \"xlm-roberta-large\"\n",
    "name_folder_and_model = 'xlm-roberta-large_30.bin'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)\n",
    "data = pd.read_csv(\"ner_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902ad843-b9a6-4aeb-b283-6590444aea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¥–∞–Ω–Ω—ã–µ —Å–ø–∞—Ä—Å–µ–Ω—ã —Å –¢–æ–ª–æ–∫–∏, –ø–æ—ç—Ç–æ–º—É –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∏—Ö –Ω—É–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å, \n",
    "# —É–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ '\\' –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–∑ str –≤ —Å–ø–∏—Å–æ–∫ dict-–æ–≤\n",
    "import json\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7c226-ba06-4b25-8e7c-1423f3d49e00",
   "metadata": {},
   "source": [
    "#### –û—Ä–∏–≥–∏–Ω–∞–ª —Ç—É—Ç–æ—Ä–∏–∞–ª–∞ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç—É—Ç https://gist.github.com/avidale/cacf235aebeaaf4c578389e1146c3c57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee336880-48fd-4c3c-9b84-4f9487d5d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–ø–µ—Ä—å –∏–∑ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–º –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω–∞) –µ–≥–æ —Ç–µ–≥ (label) –∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –ø—Ä–µ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "from razdel import tokenize\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —É–¥–æ–±–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ razdel, \n",
    "    # –æ–Ω–∞ –ø–æ–º–∏–º–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω—ã–µ –¥–ª—è –Ω–∞—Å —á–∏—Å–ª–∞ - –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "    \n",
    "    raw_toks = list(tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # –ø—Ä–∏—Å–≤–æ–∏–º –¥–ª—è –Ω–∞—á–∞–ª–∞ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —Ç–µ–≥ '–û' - —Ç–µ–≥, –æ–∑–Ω–∞—á–∞—é—â–∏–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NER-–∞\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # —Ç–∞–∫ –∫–∞–∫ NER –º–æ–∂–µ–º —Å–æ—Å—Ç–∞—è—Ç—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤, —Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä—Ü–∏—é\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43db9d6-76f9-4f24-87ce-bea724a238a9",
   "metadata": {},
   "source": [
    "### –û–±—Ä–∞–±–æ—Ç–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ —Ä–∞–∑–æ–±—å–µ–º –Ω–∞ —Ç—Ä–µ–π–Ω –∏ —Ç–µ—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea76082-efcc-44da-ba3c-7dd466d3ecb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f181346-579e-4804-975a-f6133a86a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc651f-f5f7-48f6-82d3-acfbb38a5296",
   "metadata": {},
   "source": [
    "#### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–æ–ª—É—á–∏–≤—à–∏–µ—Å—è —Ç–µ–≥–∏\n",
    "–ü–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ—á–∏—Ç–∞—Ç—å –ø—Ä–æ BIO —Ç–µ–≥–∏ –º–æ–∂–Ω–æ —Ç—É—Ç https://datascience.stackexchange.com/questions/63399/what-is-bio-tags-for-creating-custom-ner-named-entity-recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d154c93-c038-4c4a-b9ee-8dba7bd1a21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-–î–∞—Ç–∞',\n",
       " 'B-–±—Ä–µ–Ω–¥',\n",
       " 'B-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'B-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'B-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'B-–ª–∏–≥–∞',\n",
       " 'B-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'B-–º–æ–¥–µ–ª—å',\n",
       " 'B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'B-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'B-—Å–µ–∑–æ–Ω',\n",
       " 'B-—Å–µ—Ä–∏—è',\n",
       " 'I-–î–∞—Ç–∞',\n",
       " 'I-–±—Ä–µ–Ω–¥',\n",
       " 'I-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'I-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'I-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'I-–ª–∏–≥–∞',\n",
       " 'I-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'I-–º–æ–¥–µ–ª—å',\n",
       " 'I-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'I-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'I-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'I-—Å–µ–∑–æ–Ω',\n",
       " 'I-—Å–µ—Ä–∏—è']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803e14bf-5ca9-42bf-8c67-f3718b53d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4612e752-dc7d-41e6-bede-73e67319aa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5137\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 1285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd286f0c-d748-4289-bf3e-8064e45186cc",
   "metadata": {},
   "source": [
    "### –ó–∞–ø—É—Å—Ç–∏–º –º–æ–¥–µ–ª—å RuBert-tiny - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Bert, –ø–æ–≤–µ—Ä—Ö –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞–≤–µ—à–µ–Ω —Å–ª–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417623f9-c9ac-488f-9e1b-31dbf61e049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58c25f2-b144-4b0c-9dd6-76760d26152f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '–ù–ê–ó–í–ê–ù–ò–ï', ':', '>', '–ú–æ—Å–∫–æ–≤—Å–∫–∏–π', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π', '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '–º–∏—Ä–∞', '=', '89', '–∏', '–°—Ç–∞—Å', '–ù–∞–º–∏–Ω', '.', '¬´', '–ì–ª–∞–≤–Ω—ã–π', '–¥–µ–Ω—å', '¬ª', '<', '–û–ü–ò–°–ê–ù–ò–ï', ':', '>', '12', '–∏', '13', '–∏—é–Ω—è', '1989', '–≥–æ–¥–∞', '–Ω–∞', '–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º', '—Å—Ç–∞–¥–∏–æ–Ω–µ', '–∏–º–µ–Ω–∏', '–í', '.', '–ò', '.', '–õ–µ–Ω–∏–Ω–∞', '—Å–æ—Å—Ç–æ—è–ª—Å—è', '–ø–µ—Ä–≤—ã–π', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π', '—Ä–æ–∫', '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '.', '–ö–æ–Ω—Ü–µ—Ä—Ç', '—Å–æ–±—Ä–∞–ª', '—Å–≤—ã—à–µ', '100', '—Ç—ã—Å—è—á', '–∑—Ä–∏—Ç–µ–ª–µ–π', '.', '–°–æ–≤–µ—Ç—Å–∫–∞—è', '–º–æ–ª–æ–¥–µ–∂—å', '—Ç–æ–≥–¥–∞', '—É–≤–∏–¥–µ–ª–∞', '–∏', '—É—Å–ª—ã—à–∞–ª–∞', '–ø—Ä–∏–∑–Ω–∞–Ω–Ω—ã—Ö', '–º–∏—Ä–æ–≤—ã—Ö', '–∫—É–º–∏—Ä–æ–≤', ':', 'Scorpions', ',', 'Bon', 'Jovi', ',', 'Ozzy', 'Osbourne', ',', 'Cinderella', ',', 'Motley', 'Crue', '.', '–ü—Ä–∏–µ–∑–¥', '–∑–∞–ø–∞–¥–Ω—ã—Ö', '–∑–≤–µ–∑–¥', '–∫–∞–∑–∞–ª—Å—è', '–æ–∂–∏–≤—à–µ–π', '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–æ–π', '.', '–§–µ—Å—Ç–∏–≤–∞–ª—å', '—Å—Ç–∞–ª', '–Ω–µ', '—Ç–æ–ª—å–∫–æ', '–º—É–∑—ã–∫–∞–ª—å–Ω–æ–π', ',', '–Ω–æ', '–∏', '–≤–∞–∂–Ω–æ–π', '–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π', '–∞–∫—Ü–∏–µ–π', '.', '–í—Å—è', '–º–∏—Ä–æ–≤–∞—è', '–ø—Ä–µ—Å—Å–∞', '–ø–∏—Å–∞–ª–∞', '–æ', '—Ç–æ–º', ',', '—á—Ç–æ', '–°–°–°–†', '–∏', '–°–®–ê', ',', '–¥–æ–ª–≥–æ–µ', '–≤—Ä–µ–º—è', '–Ω–∞—Ö–æ–¥–∏–≤—à–∏–µ—Å—è', '–≤', '—Å–æ—Å—Ç–æ—è–Ω–∏–∏', '\"', '—Ö–æ–ª–æ–¥–Ω–æ–π', '–≤–æ–π–Ω—ã', '\"', ',', '–Ω–∞–∫–æ–Ω–µ—Ü', '—Ç–æ', '—Å—Ç–∞–ª–∏', '–Ω–∞–ª–∞–∂–∏–≤–∞—Ç—å', '–¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '–Ω–∞', '–∫—É–ª—å—Ç—É—Ä–Ω–æ–º', '—Ñ—Ä–æ–Ω—Ç–µ', '.', '–ü—Ä–∏–µ–∑–¥', '–∑–∞–ø–∞–¥–Ω—ã—Ö', '–º—É–∑—ã–∫–∞–Ω—Ç–æ–≤', '—Ä–∞–∑—Ä—É—à–∏–ª', '–º–∏—Ñ—ã', '–∏', '—Å—Ç–µ—Ä–µ–æ—Ç–∏–ø—ã', '–æ', '–°–æ–≤–µ—Ç—Å–∫–æ–º', '–°–æ—é–∑–µ', ',', '–∫–∞–∫', '–æ', '—Å—É—Ä–æ–≤–æ–π', '–∏', '–∑–∞–∫—Ä—ã—Ç–æ–π', '—Å—Ç—Ä–∞–Ω–µ', '.', '–ù–æ', '—á—Ç–æ', '–æ—Å—Ç–∞–ª–æ—Å—å', '–∑–∞', '–∫–∞–¥—Ä–æ–º', '—ç—Ç–æ–π', '–º—É–∑—ã–∫–∞–ª—å–Ω–æ–π', '—Ä–µ–≤–æ–ª—é—Ü–∏–∏', '?', '–ì–ª–∞–≤–Ω—ã–π', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä', '—Ñ–µ—Å—Ç–∏–≤–∞–ª—è', '–º—É–∑—ã–∫–∞–Ω—Ç', '–∏', '–ø—Ä–æ–¥—é—Å–µ—Ä', '–°—Ç–∞—Å', '–ù–∞–º–∏–Ω', ',', '–∞', '—Ç–∞–∫–∂–µ', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ', '—É—á–∞—Å—Ç–Ω–∏–∫–∏', '—Å–æ–≥–ª–∞—Å–∏–ª–∏—Å—å', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è', '—Å–≤–æ–∏–º–∏', '–ª–∏—á–Ω—ã–º–∏', '–≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏', '.']\n"
     ]
    }
   ],
   "source": [
    "example = ner_train[5]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bf5c30-7ee2-46d3-9733-da3ff8b803fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '‚ñÅ<', '‚ñÅ–ù–ê', '–ó', '–í–ê', '–ù–ò–ï', '‚ñÅ:', '‚ñÅ>', '‚ñÅ–ú–æ—Å–∫–æ–≤—Å–∫', '–∏–π', '‚ñÅ–º–µ–∂–¥—É', '–Ω–∞—Ä–æ–¥', '–Ω—ã–π', '‚ñÅ—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '‚ñÅ–º–∏—Ä–∞', '‚ñÅ=', '‚ñÅ89', '‚ñÅ–∏', '‚ñÅ–°—Ç–∞', '—Å', '‚ñÅ–ù–∞', '–º–∏–Ω', '‚ñÅ', '.', '‚ñÅ¬´', '‚ñÅ–ì–ª–∞–≤', '–Ω—ã–π', '‚ñÅ–¥–µ–Ω—å', '‚ñÅ¬ª', '‚ñÅ<', '‚ñÅ–û', '–ü–ò–°', '–ê', '–ù–ò–ï', '‚ñÅ:', '‚ñÅ>', '‚ñÅ12', '‚ñÅ–∏', '‚ñÅ13', '‚ñÅ–∏—é–Ω—è', '‚ñÅ1989', '‚ñÅ–≥–æ–¥–∞', '‚ñÅ–Ω–∞', '‚ñÅ–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω', '–æ–º', '‚ñÅ—Å—Ç–∞–¥–∏–æ–Ω', '–µ', '‚ñÅ–∏–º–µ–Ω–∏', '‚ñÅ–í', '‚ñÅ', '.', '‚ñÅ–ò', '‚ñÅ', '.', '‚ñÅ–õ–µ–Ω–∏–Ω', '–∞', '‚ñÅ—Å–æ—Å—Ç–æ—è', '–ª—Å—è', '‚ñÅ–ø–µ—Ä–≤—ã–π', '‚ñÅ–º–µ–∂–¥—É', '–Ω–∞—Ä–æ–¥', '–Ω—ã–π', '‚ñÅ—Ä–æ–∫', '‚ñÅ—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '‚ñÅ', '.', '‚ñÅ–ö–æ–Ω—Ü–µ—Ä—Ç', '‚ñÅ—Å–æ', '–±—Ä–∞–ª', '‚ñÅ—Å–≤—ã—à–µ', '‚ñÅ100', '‚ñÅ—Ç—ã—Å—è—á', '‚ñÅ–∑—Ä–∏—Ç–µ–ª', '–µ–π', '‚ñÅ', '.', '‚ñÅ–°–æ–≤–µ—Ç', '—Å–∫–∞—è', '‚ñÅ–º–æ–ª–æ–¥', '–µ–∂', '—å', '‚ñÅ—Ç–æ–≥–¥–∞', '‚ñÅ—É–≤–∏–¥–µ–ª', '–∞', '‚ñÅ–∏', '‚ñÅ—É—Å–ª—ã—à–∞–ª', '–∞', '‚ñÅ–ø—Ä–∏–∑–Ω–∞', '–Ω–Ω—ã—Ö', '‚ñÅ–º–∏—Ä', '–æ–≤—ã—Ö', '‚ñÅ–∫—É', '–º–∏—Ä', '–æ–≤', '‚ñÅ:', '‚ñÅS', 'corp', 'ions', '‚ñÅ', ',', '‚ñÅBon', '‚ñÅJo', 'vi', '‚ñÅ', ',', '‚ñÅO', 'zzy', '‚ñÅOs', 'bour', 'ne', '‚ñÅ', ',', '‚ñÅC', 'inde', 'rella', '‚ñÅ', ',', '‚ñÅMot', 'ley', '‚ñÅCru', 'e', '‚ñÅ', '.', '‚ñÅ–ü—Ä–∏', '–µ–∑–¥', '‚ñÅ–∑–∞–ø–∞–¥', '–Ω—ã—Ö', '‚ñÅ–∑–≤–µ–∑–¥', '‚ñÅ–∫–∞–∑–∞', '–ª—Å—è', '‚ñÅ–æ–∂–∏', '–≤—à–µ–π', '‚ñÅ—Ñ–∞–Ω—Ç–∞—Å—Ç–∏', '–∫–æ–π', '‚ñÅ', '.', '‚ñÅ–§–µ—Å—Ç–∏–≤–∞–ª', '—å', '‚ñÅ—Å—Ç–∞–ª', '‚ñÅ–Ω–µ', '‚ñÅ—Ç–æ–ª—å–∫–æ', '‚ñÅ–º—É–∑—ã–∫–∞', '–ª—å–Ω–æ–π', '‚ñÅ', ',', '‚ñÅ–Ω–æ', '‚ñÅ–∏', '‚ñÅ–≤–∞–∂–Ω–æ', '–π', '‚ñÅ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π', '‚ñÅ–∞–∫', '—Ü–∏–µ–π', '‚ñÅ', '.', '‚ñÅ–í', '—Å—è', '‚ñÅ–º–∏—Ä', '–æ–≤–∞—è', '‚ñÅ–ø—Ä–µ—Å—Å', '–∞', '‚ñÅ–ø–∏—Å–∞–ª', '–∞', '‚ñÅ–æ', '‚ñÅ—Ç–æ–º', '‚ñÅ', ',', '‚ñÅ—á—Ç–æ', '‚ñÅ–°–°–°–†', '‚ñÅ–∏', '‚ñÅ–°–®–ê', '‚ñÅ', ',', '‚ñÅ–¥–æ–ª–≥–æ', '–µ', '‚ñÅ–≤—Ä–µ–º—è', '‚ñÅ–Ω–∞—Ö–æ–¥–∏', '–≤—à–∏–µ—Å—è', '‚ñÅ–≤', '‚ñÅ—Å–æ—Å—Ç–æ—è–Ω–∏–∏', '‚ñÅ\"', '‚ñÅ—Ö–æ–ª–æ–¥–Ω–æ', '–π', '‚ñÅ–≤–æ–π–Ω—ã', '‚ñÅ\"', '‚ñÅ', ',', '‚ñÅ–Ω–∞–∫–æ–Ω–µ—Ü', '‚ñÅ—Ç–æ', '‚ñÅ—Å—Ç–∞–ª–∏', '‚ñÅ–Ω–∞', '–ª–∞', '–∂–∏–≤–∞—Ç—å', '‚ñÅ', '–¥—Ä—É–∂–µ', '—Å—Ç–≤–µ–Ω', '–Ω—ã–µ', '‚ñÅ–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '‚ñÅ–Ω–∞', '‚ñÅ–∫—É–ª—å—Ç—É—Ä', '–Ω–æ–º', '‚ñÅ—Ñ—Ä–æ–Ω—Ç', '–µ', '‚ñÅ', '.', '‚ñÅ–ü—Ä–∏', '–µ–∑–¥', '‚ñÅ–∑–∞–ø–∞–¥', '–Ω—ã—Ö', '‚ñÅ–º—É–∑—ã–∫–∞–Ω—Ç', '–æ–≤', '‚ñÅ—Ä–∞–∑—Ä—É—à–∏', '–ª', '‚ñÅ–º–∏', '—Ñ', '—ã', '‚ñÅ–∏', '‚ñÅ—Å—Ç–µ—Ä–µ–æ—Ç–∏–ø', '—ã', '‚ñÅ–æ', '‚ñÅ–°–æ–≤–µ—Ç', '—Å–∫–æ–º', '‚ñÅ–°–æ—é–∑', '–µ', '‚ñÅ', ',', '‚ñÅ–∫–∞–∫', '‚ñÅ–æ', '‚ñÅ—Å—É—Ä', '–æ–≤–æ–π', '‚ñÅ–∏', '‚ñÅ–∑–∞–∫—Ä—ã—Ç', '–æ–π', '‚ñÅ—Å—Ç—Ä–∞–Ω–µ', '‚ñÅ', '.', '‚ñÅ–ù–æ', '‚ñÅ—á—Ç–æ', '‚ñÅ–æ—Å—Ç–∞–ª–æ—Å—å', '‚ñÅ–∑–∞', '‚ñÅ–∫–∞–¥—Ä', '–æ–º', '‚ñÅ—ç—Ç–æ–π', '‚ñÅ–º—É–∑—ã–∫–∞', '–ª—å–Ω–æ–π', '‚ñÅ—Ä–µ–≤–æ–ª—é—Ü–∏–∏', '‚ñÅ?', '‚ñÅ–ì–ª–∞–≤', '–Ω—ã–π', '‚ñÅ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä', '‚ñÅ—Ñ–µ—Å—Ç–∏–≤–∞–ª', '—è', '‚ñÅ–º—É–∑—ã–∫–∞–Ω—Ç', '‚ñÅ–∏', '‚ñÅ–ø—Ä–æ–¥—é—Å–µ—Ä', '‚ñÅ–°—Ç–∞', '—Å', '‚ñÅ–ù–∞', '–º–∏–Ω', '‚ñÅ', ',', '‚ñÅ–∞', '‚ñÅ—Ç–∞–∫–∂–µ', '‚ñÅ–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ', '‚ñÅ—É—á–∞—Å—Ç–Ω–∏–∫–∏', '‚ñÅ—Å–æ–≥–ª–∞—Å–∏', '–ª–∏—Å—å', '‚ñÅ–ø–æ–¥–µ–ª–∏', '—Ç—å—Å—è', '‚ñÅ—Å–≤–æ–∏–º–∏', '‚ñÅ–ª–∏', '—á–Ω—ã–º–∏', '‚ñÅ–≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è', '–º–∏', '‚ñÅ', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4163cecf-a14b-4c26-9273-3fa400055163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 276)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2321f2-8b2e-4178-bbd8-0b0a803aedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 1, 1, 2, 3, 4, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 11, 12, 12, 13, 13, 14, 15, 15, 16, 17, 18, 19, 19, 19, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 30, 31, 32, 33, 33, 34, 35, 35, 36, 36, 37, 37, 38, 39, 39, 39, 40, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 48, 49, 49, 50, 50, 51, 51, 51, 52, 53, 53, 54, 55, 55, 56, 56, 57, 57, 58, 58, 58, 59, 60, 60, 60, 61, 61, 62, 63, 63, 64, 64, 65, 65, 66, 66, 66, 67, 67, 68, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 73, 74, 74, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 82, 83, 84, 84, 85, 85, 86, 87, 88, 88, 89, 90, 90, 91, 91, 92, 92, 93, 93, 94, 94, 95, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 103, 104, 104, 105, 106, 106, 107, 108, 109, 110, 110, 111, 112, 113, 113, 114, 115, 116, 117, 117, 117, 118, 118, 118, 118, 119, 120, 121, 121, 122, 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 128, 129, 130, 130, 131, 132, 132, 133, 133, 134, 134, 135, 136, 137, 137, 138, 139, 139, 140, 141, 141, 142, 143, 144, 145, 146, 146, 147, 148, 148, 149, 150, 151, 151, 152, 153, 153, 154, 155, 156, 157, 157, 158, 158, 159, 159, 160, 161, 162, 163, 164, 164, 165, 165, 166, 167, 167, 168, 168, 169, 169, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17363352-e562-450a-a54f-f30cf6a3ede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 276\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa21220-e351-4259-83c5-c60cdb73dfe2",
   "metadata": {},
   "source": [
    "#### –£ Bert —Å–≤–æ–π —Å–æ–±—Å–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ –º–µ–ª–∫–∏–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω—É–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–µ –∏–º –Ω–µ—Ä—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cb5952-a6aa-4032-8940-ec397d2bb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2735bc96-92cc-4361-8c44-583eabf4bb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 4426, 5315, 4026, 29889, 189182, 152, 977, 51208, 75586, 54039, 6, 83389, 45799, 1270, 102356, 4426, 1089, 157933, 1709, 189182, 152, 977, 123811, 336, 10698, 4558, 8568, 111529, 89, 69361, 19776, 100347, 292, 51208, 75586, 54039, 6, 83389, 45799, 1270, 102356, 6, 5, 417, 3920, 8165, 47745, 90216, 10041, 13549, 4476, 6, 5, 1089, 25358, 98590, 35, 98465, 100347, 1214, 129, 49571, 546, 135, 170769, 146408, 131567, 6, 47871, 1435, 8113, 1560, 8568, 129, 184862, 312, 1089, 88691, 89, 93362, 419, 93834, 17968, 63781, 6, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_data['train'][1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39acdcaf-7f6b-467f-a143-9820b4f13ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bb2a708c1d4b6db895c16adeb103cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257a26f56206480d8f3672e48bb71bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce292-6c29-43be-909b-adb1536b6ebf",
   "metadata": {},
   "source": [
    "#### –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä–∏–∫ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤–∏—è —Ç–µ–≥–∞ –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edcbfc0-6bbd-4be1-9da7-6fe6e348753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7118790-3483-4fca-b3d9-950287202381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–µ–π\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbfb44-68cc-42d1-98da-1a12a87aac10",
   "metadata": {},
   "source": [
    "### –í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫ –≤–æ–∑—å–º–µ–º precision, recall, accuracy, –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥ Ner –∑–∞–¥–∞—á—É –±–∏–±–ª–∏–æ—Ç–µ–∫—É seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc2483f-93f8-4207-92e2-1f91217a42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19660\\152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73d999fc-9eeb-4cd3-b358-762fdfd80f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'–±—Ä–µ–Ω–¥': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " '–ª–æ–∫–∞—Ü–∏—è': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_train[4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac26e3f4-52f8-4d98-b629-cd2b32fd8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1dbfdd9-443e-4b5e-90a7-e9abac9d4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='643' max='643' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [643/643 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.375157356262207,\n",
       " 'eval_precision': 0.004121586810922205,\n",
       " 'eval_recall': 0.0381474949376184,\n",
       " 'eval_f1': 0.007439395676488197,\n",
       " 'eval_accuracy': 0.009479054211384178,\n",
       " 'eval_runtime': 35.7511,\n",
       " 'eval_samples_per_second': 35.943,\n",
       " 'eval_steps_per_second': 17.985}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# —á—Ç–æ –º—ã –≤–∏–¥–∏–º –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9441d5bb-9bf3-4f3d-ad2d-24e577ed5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers.trainer import logger as noisy_logger\n",
    "noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf85ec22-19e5-4e10-be46-bf99fb57a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–µ—Ä—Ç–∞ –º–æ–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –∑–∞–º–æ—Ä–æ–∑–∫–æ–π/—Ä–∞–∑–º–æ—Ä–æ–∑–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, –∑–¥–µ—Å—å –º—ã –æ—Å—Ç–∞–≤–∏–º –≤—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ \n",
    "# –î–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å—é –±–µ—Ä—Ç–æ–≤—É—é —á–∞—Å—Ç—å, –∫—Ä–æ–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –Ω–æ —Ç–æ–≥–¥–∞ –∫–∞—á–µ—Å–≤—Ç–æ –±—É–¥–µ—Ç –ø–æ—Ö—É–∂–µ\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b907071-d37c-4d5b-8ce6-57aea13663bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'max_split_size_mb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_split_size_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'max_split_size_mb'"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5195e9a2-2c31-47f0-b539-512c486e99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90d12a5c-639b-4d84-a8c0-92b52b3cdf60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.17 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\transformers\\trainer.py:1971\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1969\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1972\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\accelerate\\optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\adamw.py:160\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\projects\\2023-10-hackaton-rutube\\dataset\\venv\\Lib\\site-packages\\torch\\optim\\adamw.py:118\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    114\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    115\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    124\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    125\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.17 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a36b49-d3d3-4dcc-bb33-5b3cbdb555ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edf3c3-70e7-49c4-a9b6-842a0eb4f014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# –ü–æ—Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f349-9ba5-42fe-8f5f-780c6f26d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead2c66-11d0-4cb8-ab58-530d2524b941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd985f-b4f6-4e6f-b28a-a54bf2e59c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(name_folder_and_model)\n",
    "tokenizer.save_pretrained(name_folder_and_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea14342-bbf6-40c0-aa44-3c2db9c8215b",
   "metadata": {},
   "source": [
    "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519aaaf-02b8-4f66-885c-8930073508a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ' '.join(ner_train[25]['tokens'])\n",
    "text = ner_train[25]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee5bc1-634c-4bde-aa51-64425aba5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device='cpu')\n",
    "\n",
    "def predict_ner(text, tokenizer, model, pipe, verbose=True):\n",
    "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens)\n",
    "    # print(pred.logits.shape)\n",
    "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    labels = []\n",
    "    for t, idx in zip(token_text, indices):\n",
    "        if '##' not in t:\n",
    "            labels.append(label_list[idx])\n",
    "        if verbose:    \n",
    "            print(f'{t:15s} {label_list[idx]:10s}')\n",
    "    return text, pipe(text), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a3a44-a23d-4615-8885-63da60761d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ner(text, tokenizer, model, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f580a-4542-42ea-98f4-de4a89fed35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b333764e-e82d-4c8f-9f89-8390f92da7cb",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –±—É–¥–µ—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ, –Ω–æ –ø—Ä–æ–≥–æ–Ω–∏–º –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ —á–µ—Ä–µ–∑ –Ω–∞—à—É –æ—Ç–ª–æ–∂–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "–í–ê–ñ–ù–û: –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —É –≤–∞—Å –±—É–¥–µ—Ç —Ç–µ—Å—Ç –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ, —á—Ç–æ –æ–Ω –±—ã–ª –≤ —Ç—Ä–µ–π–Ω–µ 'video_info', –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–∞–±–º–∏—à–µ–Ω–µ —ç—Ç—É –∫–æ–ª–æ–Ω–∫—É –∏ –∏–Ω–¥–µ–∫—Å—ã –º–µ–Ω—è—Ç—å –Ω–µ–ª—å–∑—è, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É 'entities_prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ccfdd-46aa-49a0-af51-c7adea1d0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "submission = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
    "submission['entities_prediction'] = submission['entities_prediction'].astype('object')\n",
    "def sample_submission(text, tokenizer, model, pipe, submission):\n",
    "    for i, elem in enumerate(ner_test):\n",
    "        _, _, labels = predict_ner(elem['tokens'], tokenizer, model, pipe, verbose=False)\n",
    "        submission.loc[i, 'video_info'] = elem\n",
    "\n",
    "        submission.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958310b4-09fa-41bd-b888-b4364be3f397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = sample_submission(text, tokenizer, model, pipe, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0588b1-6e42-4f37-bfe6-2f695cb6971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdcc7-3750-46ad-abea-36fc712feb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90948bec-8675-46a0-8e6b-df159b8e4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dcd75-30bc-4bac-9d5c-a5ee879bd3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
