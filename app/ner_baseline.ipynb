{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ae1834-ae30-4799-8aa4-5786ee49d845",
   "metadata": {},
   "source": [
    "### Baseline модель для определения именованных сущностей по кейсу от Rutube.\n",
    "Поскольку нам нужно распознать нестандартные NER, можно воспользоваться помощью языковых моделей, в данном случае - Bert.\n",
    "Данные вы уже получили  - это разметка, сделанная на Толоке, она не идеальна, но это часть практической задачи, с которой можно столкнуться в реальности. \n",
    "\n",
    "Небольшое введение в NER https://habr.com/ru/companies/contentai/articles/449514/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f651cc-6e8c-431c-a84e-2e7d310c9d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from transformers[torch]) (0.24.0)\n",
      "Requirement already satisfied: psutil in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: fsspec in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.10->transformers[torch]) (12.3.52)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: seqeval in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: corus in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: razdel in /home/tommy/git-projects/2023-10-hackaton-rutube/dataset/venv/lib/python3.11/site-packages (0.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/619.9 MB\u001b[0m \u001b[31m147.6 kB/s\u001b[0m eta \u001b[36m1:09:42\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] \n",
    "!pip install datasets seqeval\n",
    "!pip install corus razdel \n",
    "!pip install torch==2.0.0\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86789ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем данные\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"ner_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105af13-906b-43cc-b795-bdae96d030ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ad843-b9a6-4aeb-b283-6590444aea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные спарсены с Толоки, поэтому могут иметь проблемы с символами и их нужно избежать, \n",
    "# удалить лишние '\\' например, преобразовать из str в список dict-ов\n",
    "import json\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768495c-c2a5-4c74-a5de-a60bbd60898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7c226-ba06-4b25-8e7c-1423f3d49e00",
   "metadata": {},
   "source": [
    "#### Оригинал туториала на медицинских данных можно посмотреть тут https://gist.github.com/avidale/cacf235aebeaaf4c578389e1146c3c57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee336880-48fd-4c3c-9b84-4f9487d5d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь из наших данных нам нужно извлечь для каждого слова (токена) его тег (label) из разметки, чтобы потом предать в модель классификации токенов\n",
    "from razdel import tokenize\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # воспользуемся удобным токенайзером из библиотеки razdel, \n",
    "    # она помимо разбиения на слова, сохраняет важные для нас числа - начало и конец слова в токенах\n",
    "    \n",
    "    raw_toks = list(tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # присвоим для начала каждому слову тег 'О' - тег, означающий отсутствие NER-а\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # так как NER можем состаять из нескольких слов, то нам нужно сохранить эту инфорцию\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != 'не найдено':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d3224-bd70-4a99-bdb3-fee8945ce2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_labels(df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43db9d6-76f9-4f24-87ce-bea724a238a9",
   "metadata": {},
   "source": [
    "### Обработаем датасет и разобьем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea76082-efcc-44da-ba3c-7dd466d3ecb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f181346-579e-4804-975a-f6133a86a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.DataFrame(ner_train).sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc651f-f5f7-48f6-82d3-acfbb38a5296",
   "metadata": {},
   "source": [
    "#### Посмотрим на получившиеся теги\n",
    "Подробнее почитать про BIO теги можно тут https://datascience.stackexchange.com/questions/63399/what-is-bio-tags-for-creating-custom-ner-named-entity-recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d154c93-c038-4c4a-b9ee-8dba7bd1a21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e14bf-5ca9-42bf-8c67-f3718b53d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612e752-dc7d-41e6-bede-73e67319aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd286f0c-d748-4289-bf3e-8064e45186cc",
   "metadata": {},
   "source": [
    "### Запустим модель RuBert-tiny - классический Bert, поверх которого навешен слой классификации токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417623f9-c9ac-488f-9e1b-31dbf61e049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "model_checkpoint = \"cointegrated/rubert-tiny\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c25f2-b144-4b0c-9dd6-76760d26152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ner_train[5]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf5c30-7ee2-46d3-9733-da3ff8b803fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163cecf-a14b-4c26-9273-3fa400055163",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example[\"tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2321f2-8b2e-4178-bbd8-0b0a803aedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17363352-e562-450a-a54f-f30cf6a3ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa21220-e351-4259-83c5-c60cdb73dfe2",
   "metadata": {},
   "source": [
    "#### У Bert свой собсвенный токенайзер, который разбивает слова на мелкие токены, поэтому нам нужно корректно сопоставить токены и соответсвующие им неры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb5952-a6aa-4032-8940-ec397d2bb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735bc96-92cc-4361-8c44-583eabf4bb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize_and_align_labels(ner_data['train'][1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acdcaf-7f6b-467f-a143-9820b4f13ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce292-6c29-43be-909b-adb1536b6ebf",
   "metadata": {},
   "source": [
    "#### Сохраняем словарик соотвествия тега и его индекса внутри модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcbfc0-6bbd-4be1-9da7-6fe6e348753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7118790-3483-4fca-b3d9-950287202381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Специальный объект для удобного формирования батчей\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbfb44-68cc-42d1-98da-1a12a87aac10",
   "metadata": {},
   "source": [
    "### В качестве метрик возьмем precision, recall, accuracy, для этого воспользуемся специализированной под Ner задачу библиотеку seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2483f-93f8-4207-92e2-1f91217a42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d999fc-9eeb-4cd3-b358-762fdfd80f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ner_train[4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26e3f4-52f8-4d98-b629-cd2b32fd8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbfdd9-443e-4b5e-90a7-e9abac9d4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# что мы видим без дообучения модели\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441d5bb-9bf3-4f3d-ad2d-24e577ed5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers.trainer import logger as noisy_logger\n",
    "noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85ec22-19e5-4e10-be46-bf99fb57a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для дообучения берта можно эксперементировать с заморозкой/разморозкой разных слоев, здесь мы оставим все слои размороженными \n",
    "# Для быстроты обучения можно заморозить всю бертовую часть, кроме классификатора, но тогда качесвто будет похуже\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b907071-d37c-4d5b-8ce6-57aea13663bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195e9a2-2c31-47f0-b539-512c486e99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d12a5c-639b-4d84-a8c0-92b52b3cdf60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a36b49-d3d3-4dcc-bb33-5b3cbdb555ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edf3c3-70e7-49c4-a9b6-842a0eb4f014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Посчитаем метрики на отложенном датасете\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f349-9ba5-42fe-8f5f-780c6f26d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead2c66-11d0-4cb8-ab58-530d2524b941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd985f-b4f6-4e6f-b28a-a54bf2e59c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('ner_bert.bin')\n",
    "tokenizer.save_pretrained('ner_bert.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea14342-bbf6-40c0-aa44-3c2db9c8215b",
   "metadata": {},
   "source": [
    "### Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519aaaf-02b8-4f66-885c-8930073508a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ' '.join(ner_train[25]['tokens'])\n",
    "text = ner_train[25]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee5bc1-634c-4bde-aa51-64425aba5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device='cpu')\n",
    "\n",
    "def predict_ner(text, tokenizer, model, pipe, verbose=True):\n",
    "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens)\n",
    "    # print(pred.logits.shape)\n",
    "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    labels = []\n",
    "    for t, idx in zip(token_text, indices):\n",
    "        if '##' not in t:\n",
    "            labels.append(label_list[idx])\n",
    "        if verbose:    \n",
    "            print(f'{t:15s} {label_list[idx]:10s}')\n",
    "    return text, pipe(text), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a3a44-a23d-4615-8885-63da60761d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ner(text, tokenizer, model, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f580a-4542-42ea-98f4-de4a89fed35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b333764e-e82d-4c8f-9f89-8390f92da7cb",
   "metadata": {},
   "source": [
    "### Тестового датасета у вас пока нет, по которому будет считаться метрика на лидерборде, но прогоним для примера через нашу отложенную выборку, чтобы понять формат выходных данных.\n",
    "ВАЖНО: в тестовом датасете у вас будет тест в том же формате, что он был в трейне 'video_info', в финальном сабмишене эту колонку и индексы менять нельзя, нужно будет только заполнить колонку 'entities_prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ccfdd-46aa-49a0-af51-c7adea1d0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "submission = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
    "submission['entities_prediction'] = submission['entities_prediction'].astype('object')\n",
    "def sample_submission(text, tokenizer, model, pipe, submission):\n",
    "    for i, elem in tqdm(enumerate(ner_test)):\n",
    "        _, _, labels = predict_ner(elem['tokens'], tokenizer, model, pipe, verbose=False)\n",
    "        submission.loc[i, 'video_info'] = elem\n",
    "\n",
    "        submission.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958310b4-09fa-41bd-b888-b4364be3f397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = sample_submission(text, tokenizer, model, pipe, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0588b1-6e42-4f37-bfe6-2f695cb6971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cdcc7-3750-46ad-abea-36fc712feb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ner_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
